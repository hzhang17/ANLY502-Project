{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for Koninklijke Philips NV (PHIA)\n",
    "# Manufacturing Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jingjie_ma/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.compat.v1 as tfc\n",
    "tfc.disable_v2_behavior()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pd.read_csv(\"PHIA.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingjie_ma/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# function to normalize the price\n",
    "def normalize_stock_price(df):\n",
    "    min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    df['StartPrice'] = min_max_scaler.fit_transform(df[\"StartPrice\"].values.reshape(-1,1))\n",
    "    df['MaxPrice'] = min_max_scaler.fit_transform(df[\"MaxPrice\"].values.reshape(-1,1))\n",
    "    df['MinPrice'] = min_max_scaler.fit_transform(df[\"MinPrice\"].values.reshape(-1,1))\n",
    "    df['EndPrice'] = min_max_scaler.fit_transform(df['EndPrice'].values.reshape(-1,1))\n",
    "    return df\n",
    "\n",
    "\n",
    "# create train, test, validation sets\n",
    "def split_sets(stock, seq_len):\n",
    "    # change to array\n",
    "    data_raw = stock.as_matrix() \n",
    "    # create empty data\n",
    "    data = []\n",
    "    \n",
    "    # split Data\n",
    "    valid_set_size = 0.1\n",
    "    test_set_size = 0.1\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len): \n",
    "        # append data\n",
    "        data.append(data_raw[index: index + seq_len])\n",
    "    # change to numpy array\n",
    "    data = np.array(data);\n",
    "    \n",
    "    valid_set_size = int(np.round(valid_set_size * data.shape[0]));  \n",
    "    test_set_size = int(np.round(test_set_size * data.shape[0]));\n",
    "    train_set_size = data.shape[0] - (valid_set_size + test_set_size);\n",
    "    \n",
    "    # training set\n",
    "    x_train = data[:train_set_size,:-1,:]\n",
    "    y_train = data[:train_set_size,-1,:]\n",
    "    \n",
    "    # validation set\n",
    "    x_valid = data[train_set_size:train_set_size+valid_set_size,:-1,:]\n",
    "    y_valid = data[train_set_size:train_set_size+valid_set_size,-1,:]\n",
    "    \n",
    "    # test set\n",
    "    x_test = data[train_set_size+valid_set_size:,:-1,:]\n",
    "    y_test = data[train_set_size+valid_set_size:,-1,:]\n",
    "    \n",
    "    return [x_train, y_train, x_valid, y_valid, x_test, y_test]\n",
    "\n",
    "\n",
    "# drop cols that are not going to use in this model\n",
    "df_stock = df.copy()\n",
    "df_stock.drop(['Mnemonic'],1,inplace=True)\n",
    "df_stock.drop(['Date'],1,inplace=True)\n",
    "\n",
    "\n",
    "# normalize stock prices\n",
    "df_stock_norm = df_stock.copy()\n",
    "df_stock_norm = normalize_stock_price(df_stock_norm)\n",
    "\n",
    "# create train, test data\n",
    "# chose sequence length\n",
    "seq_len = 20 \n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_sets(df_stock_norm, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use basic cell in tensorflow\n",
    "index_in_epoch = 0;\n",
    "perm_array  = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "# function to get the next batch\n",
    "def get_next_batch(batch_size):\n",
    "    global index_in_epoch, x_train, perm_array   \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    if index_in_epoch > x_train.shape[0]:\n",
    "        # random shuffle\n",
    "        np.random.shuffle(perm_array) \n",
    "        # start epoch\n",
    "        start = 0 \n",
    "        index_in_epoch = batch_size\n",
    "        \n",
    "    end = index_in_epoch\n",
    "    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n",
    "\n",
    "\n",
    "# define parameters for rnn\n",
    "n_steps = seq_len-1 \n",
    "n_inputs = 4 \n",
    "n_neurons = 300 \n",
    "n_outputs = 4\n",
    "n_layers = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "n_epochs = 100 \n",
    "train_set_size = x_train.shape[0]\n",
    "test_set_size = x_test.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "ops.reset_default_graph()\n",
    "# create placeholder\n",
    "X = tfc.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tfc.placeholder(tf.float32, [None, n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable the tf ver. 2 behavior to continue\n",
    "tfc.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-c631ad681254>:3: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "# use Basic RNN Cell\n",
    "layers = [tf.compat.v1.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)\n",
    "          for layer in range(n_layers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-547d2b665919>:1: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "multi_layer_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-e2d9bf504252>:1: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/jingjie_ma/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:456: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/jingjie_ma/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:460: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "rnn_outputs, states = tf.compat.v1.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-9283380df315>:1: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/jingjie_ma/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "stacked_outputs = tf.compat.v1.layers.dense(stacked_rnn_outputs, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get output\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "outputs = outputs[:,n_steps-1,:] \n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean square error\n",
    "mse = tf.reduce_mean(tf.square(outputs - y)) \n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate) \n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 epochs: MSE on training set/validation set = 0.161899/0.124204\n",
      "5.00 epochs: MSE on training set/validation set = 0.729387/0.912674\n",
      "10.00 epochs: MSE on training set/validation set = 0.128253/0.100558\n",
      "15.00 epochs: MSE on training set/validation set = 0.083117/0.033355\n",
      "20.00 epochs: MSE on training set/validation set = 0.063693/0.059357\n",
      "25.00 epochs: MSE on training set/validation set = 0.061648/0.052795\n",
      "30.00 epochs: MSE on training set/validation set = 0.088136/0.089492\n",
      "35.00 epochs: MSE on training set/validation set = 0.026485/0.019736\n",
      "40.00 epochs: MSE on training set/validation set = 0.058741/0.060791\n",
      "45.00 epochs: MSE on training set/validation set = 0.016643/0.023359\n",
      "50.00 epochs: MSE on training set/validation set = 0.040342/0.074167\n",
      "55.00 epochs: MSE on training set/validation set = 0.013127/0.025171\n",
      "60.00 epochs: MSE on training set/validation set = 0.023321/0.011965\n",
      "65.00 epochs: MSE on training set/validation set = 0.016224/0.009489\n",
      "70.00 epochs: MSE on training set/validation set = 0.012350/0.023929\n",
      "75.00 epochs: MSE on training set/validation set = 0.016255/0.034210\n",
      "80.00 epochs: MSE on training set/validation set = 0.009612/0.021800\n",
      "85.00 epochs: MSE on training set/validation set = 0.009957/0.010620\n",
      "90.00 epochs: MSE on training set/validation set = 0.011074/0.007020\n",
      "95.00 epochs: MSE on training set/validation set = 0.007273/0.013265\n"
     ]
    }
   ],
   "source": [
    "# recurrent neural nets\n",
    "with tf.compat.v1.Session() as sess: \n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for iteration in range(int(n_epochs*train_set_size/batch_size)):\n",
    "        x_batch, y_batch = get_next_batch(batch_size) \n",
    "        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \n",
    "        if iteration % int(5*train_set_size/batch_size) == 0:\n",
    "            mse_train = mse.eval(feed_dict={X: x_train, y: y_train}) \n",
    "            mse_valid = mse.eval(feed_dict={X: x_valid, y: y_valid}) \n",
    "            print('%.2f epochs: MSE on training set/validation set = %.6f/%.6f'%(\n",
    "                iteration*batch_size/train_set_size, mse_train, mse_valid))\n",
    "        y_train_pred = sess.run(outputs, feed_dict={X: x_train})\n",
    "        y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\n",
    "        y_test_pred = sess.run(outputs, feed_dict={X: x_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

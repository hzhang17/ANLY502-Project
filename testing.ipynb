{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-12-20.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4ee0a6f4d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a sample test data from s3, contains data from 2018-07-25 and 2018-07-26\n",
    "df = spark.read.format('csv')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load('s3://hzhang502/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+--------+----------+-------------------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|Currency|SecurityID|               Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|TradedVolume|NumberOfTrades|\n",
      "+------------+--------+--------------------+------------+--------+----------+-------------------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|DE0007472060|     WDI|         WIRECARD AG|Common stock|     EUR|   2505101|2018-07-26 00:00:00|07:00|    161.55|   161.7|   159.8|   160.2|        8823|            45|\n",
      "|DE0007164600|     SAP|         SAP SE O.N.|Common stock|     EUR|   2505077|2018-07-26 00:00:00|07:00|    101.66|  101.84|  101.66|  101.84|       53051|            46|\n",
      "|DE0006047004|     HEI|HEIDELBERGCEMENT ...|Common stock|     EUR|   2505002|2018-07-26 00:00:00|07:00|     70.88|   70.88|    70.8|   70.82|        3682|            12|\n",
      "|DE0006229107|     JEN|    JENOPTIK AG O.N.|Common stock|     EUR|   2505023|2018-07-26 00:00:00|07:00|      34.2|   34.26|   34.16|   34.16|         705|             4|\n",
      "|DE0005140008|     DBK|DEUTSCHE BANK AG ...|Common stock|     EUR|   2504888|2018-07-26 00:00:00|07:00|     10.33|   10.38|   10.29|  10.378|      320902|            67|\n",
      "+------------+--------+--------------------+------------+--------+----------+-------------------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ISIN: string (nullable = true)\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- SecurityID: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StartPrice: double (nullable = true)\n",
      " |-- MaxPrice: double (nullable = true)\n",
      " |-- MinPrice: double (nullable = true)\n",
      " |-- EndPrice: double (nullable = true)\n",
      " |-- TradedVolume: integer (nullable = true)\n",
      " |-- NumberOfTrades: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find irrelevant columns and rop\n",
    "drop = ['ISIN', 'Currency', 'SecurityID', 'TradedVolume', 'NumberOfTrades']\n",
    "for col in drop:\n",
    "    df = df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+-------------------+-----+----------+--------+--------+--------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|               Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|\n",
      "+--------+--------------------+------------+-------------------+-----+----------+--------+--------+--------+\n",
      "|     WDI|         WIRECARD AG|Common stock|2018-07-26 00:00:00|07:00|    161.55|   161.7|   159.8|   160.2|\n",
      "|     SAP|         SAP SE O.N.|Common stock|2018-07-26 00:00:00|07:00|    101.66|  101.84|  101.66|  101.84|\n",
      "|     HEI|HEIDELBERGCEMENT ...|Common stock|2018-07-26 00:00:00|07:00|     70.88|   70.88|    70.8|   70.82|\n",
      "|     JEN|    JENOPTIK AG O.N.|Common stock|2018-07-26 00:00:00|07:00|      34.2|   34.26|   34.16|   34.16|\n",
      "|     DBK|DEUTSCHE BANK AG ...|Common stock|2018-07-26 00:00:00|07:00|     10.33|   10.38|   10.29|  10.378|\n",
      "+--------+--------------------+------------+-------------------+-----+----------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136707"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of by minute record in 2 days\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Mnemonic|count|\n",
      "+--------+-----+\n",
      "|    null|    1|\n",
      "|    I8IC|    2|\n",
      "|    SPYL|    2|\n",
      "|    DVEU|    2|\n",
      "|    ZPR3|    2|\n",
      "|    VG82|    2|\n",
      "|    H4ZN|    2|\n",
      "|    UNIN|    2|\n",
      "|    TNE2|    2|\n",
      "|    FVUI|    2|\n",
      "|    INDM|    2|\n",
      "|     BYG|    2|\n",
      "|    VWSA|    2|\n",
      "|    I8IE|    2|\n",
      "|    GMMV|    2|\n",
      "|    DPWA|    2|\n",
      "|    I8IF|    2|\n",
      "|     BRH|    2|\n",
      "|    ERCA|    2|\n",
      "|    SPPL|    2|\n",
      "|    SYBX|    2|\n",
      "|    IESE|    2|\n",
      "|    NGLD|    2|\n",
      "|    ASMF|    2|\n",
      "|    WSOJ|    2|\n",
      "+--------+-----+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see what are frequently traded stocks\n",
    "# showing less results here for the sake of simplicity\n",
    "df.groupBy('Mnemonic').count().orderBy('count').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change type of Date to date\n",
    "df_new = df.withColumn('Date', df['Date'].cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only stocks\n",
    "df_new = df_new.filter(df_new['SecurityType'] == 'Common stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate time for later use\n",
    "import pyspark\n",
    "import pyspark.sql.functions as fct\n",
    "split_col = fct.split(df_new['Time'], ':')\n",
    "df1 = df_new.withColumn('Hour', split_col.getItem(0))\n",
    "df1 = df1.withColumn('Minute', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|     WDI|         WIRECARD AG|Common stock|2018-07-26|07:00|    161.55|   161.7|   159.8|   160.2|  07|    00|\n",
      "|     SAP|         SAP SE O.N.|Common stock|2018-07-26|07:00|    101.66|  101.84|  101.66|  101.84|  07|    00|\n",
      "|     HEI|HEIDELBERGCEMENT ...|Common stock|2018-07-26|07:00|     70.88|   70.88|    70.8|   70.82|  07|    00|\n",
      "|     JEN|    JENOPTIK AG O.N.|Common stock|2018-07-26|07:00|      34.2|   34.26|   34.16|   34.16|  07|    00|\n",
      "|     DBK|DEUTSCHE BANK AG ...|Common stock|2018-07-26|07:00|     10.33|   10.38|   10.29|  10.378|  07|    00|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StartPrice: double (nullable = true)\n",
      " |-- MaxPrice: double (nullable = true)\n",
      " |-- MinPrice: double (nullable = true)\n",
      " |-- EndPrice: double (nullable = true)\n",
      " |-- Hour: string (nullable = true)\n",
      " |-- Minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('Hour', df1['Hour'].cast('int'))\n",
    "df1 = df1.withColumn('Minute', df1['Minute'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data from offical trading hours\n",
    "df2 = df1.filter(df1['Hour'] >= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StartPrice: double (nullable = true)\n",
      " |-- MaxPrice: double (nullable = true)\n",
      " |-- MinPrice: double (nullable = true)\n",
      " |-- EndPrice: double (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.filter((df2['Hour'] <= 17) | ((df2['Hour'] == 17) & (df2['Minute'] <= 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|     FRA|FRAPORT AG FFM.AI...|Common stock|2018-07-25|14:00|     85.64|   85.66|   85.64|   85.66|  14|     0|\n",
      "|     ADJ|ADO PROPERTIES S....|Common stock|2018-07-25|14:00|     48.26|   48.26|   48.26|   48.26|  14|     0|\n",
      "|    SOBA|AT + T INC.      ...|Common stock|2018-07-25|14:00|     26.28|   26.28|   26.28|   26.28|  14|     0|\n",
      "|     O2D|TELEFONICA DTLD H...|Common stock|2018-07-25|14:00|      3.94|   3.943|    3.94|   3.943|  14|     0|\n",
      "|     DWS|DWS GROUP GMBH+CO...|Common stock|2018-07-25|14:00|    27.345|  27.395|  27.345|  27.395|  14|     0|\n",
      "|     CON| CONTINENTAL AG O.N.|Common stock|2018-07-25|14:00|     193.5|   193.5|   193.5|   193.5|  14|     0|\n",
      "|    ZIL2|ELRINGKLINGER AG ...|Common stock|2018-07-25|14:00|     10.93|   10.93|   10.93|   10.93|  14|     0|\n",
      "|     MRK|     MERCK KGAA O.N.|Common stock|2018-07-25|14:00|     86.14|   86.22|   86.14|   86.22|  14|     0|\n",
      "|     TTI|TOM TAILOR HLDG  ...|Common stock|2018-07-25|14:00|      6.48|    6.48|   6.475|   6.475|  14|     0|\n",
      "|     N4G|THE NAGA GROUP AG...|Common stock|2018-07-25|14:00|      2.07|    2.14|    2.07|    2.09|  14|     0|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView('df3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates = spark.sql(\"Select distinct Date from df3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2018-07-25|\n",
      "|2018-07-26|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dates.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time = spark.sql(\"Select distinct Time from df3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Time|\n",
      "+-----+\n",
      "|09:10|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#time.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates.createOrReplaceTempView('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:00|   1569.95| 1570.75| 1569.64| 1570.75|  14|     0|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:01|    1570.5|  1570.5|  1570.5|  1570.5|  14|     1|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:08|   1566.01| 1566.02| 1566.01| 1566.02|  14|     8|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:13|   1566.32| 1566.32| 1566.32| 1566.32|  14|    13|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:15|    1566.6|  1566.6| 1565.32| 1565.32|  14|    15|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:17|   1567.78| 1567.78| 1567.78| 1567.78|  14|    17|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:18|   1568.37| 1568.37| 1568.37| 1568.37|  14|    18|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:21|   1567.36| 1568.39| 1567.36| 1568.39|  14|    21|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:22|   1568.46| 1568.46| 1568.46| 1568.46|  14|    22|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|14:27|   1567.16| 1567.16| 1567.16| 1567.16|  14|    27|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use amazon to test for code\n",
    "stock1 = df3.filter(df3['Mnemonic'] == 'AMZ')\n",
    "stock1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock1.createOrReplaceTempView('stock1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max price in each day\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "window = Window.partitionBy(stock1['Date']).orderBy(df['MaxPrice'].desc())\n",
    "\n",
    "max_df = stock1.select('*', rank().over(window).alias('Max')).filter(col('Max') <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|Max|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---+\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|15:35|   1573.08| 1573.08| 1573.08| 1573.08|  15|    35|  1|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-26|13:32|   1576.74| 1578.27| 1576.74| 1578.27|  13|    32|  1|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df.createOrReplaceTempView('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the min price in each day\n",
    "window1 = Window.partitionBy(stock1['Date']).orderBy(df['MinPrice'].asc())\n",
    "\n",
    "min_df = stock1.select('*', rank().over(window1).alias('Min')).filter(col('Min') <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|Min|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---+\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|13:32|    1560.0|  1560.0| 1558.64| 1558.64|  13|    32|  1|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-26|13:53|   1556.75| 1556.75| 1556.75| 1556.75|  13|    53|  1|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df.createOrReplaceTempView('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascending order of the time in each day to find the start time\n",
    "from pyspark.sql.functions import rank, desc, col\n",
    "ranked = stock1.withColumn(\"time_rank\",rank().over(Window.partitionBy(stock1['Date']).orderBy(\"Hour\", \"Minute\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranked.createOrReplaceTempView('ranked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descending order of the time in each day to find the end time\n",
    "from pyspark.sql.functions import rank, desc, col\n",
    "rankdown = ranked.withColumn(\"time_rankdown\",rank().over(Window.partitionBy(ranked['Date']).orderBy(desc(\"Hour\"), desc(\"Minute\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rankdown.createOrReplaceTempView('rankdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---------+-------------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|time_rank|time_rankdown|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---------+-------------+\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|15:35|   1573.08| 1573.08| 1573.08| 1573.08|  15|    35|      124|            1|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-25|09:01|   1567.61| 1567.61| 1567.61| 1567.61|   9|     1|        1|          124|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-26|15:35|   1566.98| 1566.98| 1566.98| 1566.98|  15|    35|      202|            1|\n",
      "|     AMZ|AMAZON.COM INC.  ...|Common stock|2018-07-26|09:00|   1566.49| 1566.49| 1566.49| 1566.49|   9|     0|        1|          202|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select only start and end time in this dataframe\n",
    "start_end = rankdown.filter((col('time_rank') == 1) | (col('time_rankdown') == 1))\n",
    "start_end.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end.createOrReplaceTempView('start_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join stock with max\n",
    "final = spark.sql(\"\"\"SELECT s.Mnemonic, s.Date, a.MaxPrice\n",
    "FROM max a INNER JOIN stock1 s On a.Max == 1 AND s.Date == a.Date\n",
    "GROUP BY s.Mnemonic, s.Date, a.MaxPrice\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+\n",
      "|Mnemonic|      Date|MaxPrice|\n",
      "+--------+----------+--------+\n",
      "|     AMZ|2018-07-26| 1578.27|\n",
      "|     AMZ|2018-07-25| 1573.08|\n",
      "+--------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.createOrReplaceTempView('final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add min to the df\n",
    "final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, i.MinPrice\n",
    "FROM min i INNER JOIN final f On i.Min == 1 AND i.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, i.MinPrice\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------+\n",
      "|Mnemonic|      Date|MaxPrice|MinPrice|\n",
      "+--------+----------+--------+--------+\n",
      "|     AMZ|2018-07-26| 1578.27| 1556.75|\n",
      "|     AMZ|2018-07-25| 1573.08| 1558.64|\n",
      "+--------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.createOrReplaceTempView('final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start price to df\n",
    "final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, s.StartPrice\n",
    "FROM start_end s INNER JOIN final f On s.time_rank == 1 AND s.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, s.StartPrice\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------+----------+\n",
      "|Mnemonic|      Date|MaxPrice|MinPrice|StartPrice|\n",
      "+--------+----------+--------+--------+----------+\n",
      "|     AMZ|2018-07-26| 1578.27| 1556.75|   1566.49|\n",
      "|     AMZ|2018-07-25| 1573.08| 1558.64|   1567.61|\n",
      "+--------+----------+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------+----------+--------+\n",
      "|Mnemonic|      Date|MaxPrice|MinPrice|StartPrice|EndPrice|\n",
      "+--------+----------+--------+--------+----------+--------+\n",
      "|     AMZ|2018-07-26| 1578.27| 1556.75|   1566.49| 1566.98|\n",
      "|     AMZ|2018-07-25| 1573.08| 1558.64|   1567.61| 1573.08|\n",
      "+--------+----------+--------+--------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add end price to df\n",
    "final.createOrReplaceTempView('final')\n",
    "final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, f.StartPrice, s.EndPrice\n",
    "FROM start_end s INNER JOIN final f On s.time_rankdown == 1 AND s.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, f.StartPrice, s.EndPrice\n",
    "\"\"\")\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the data\n",
    "final = final.orderBy('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final.write.csv('s3://hzhang502/amz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------+----------+--------+\n",
      "|Mnemonic|      Date|MaxPrice|MinPrice|StartPrice|EndPrice|\n",
      "+--------+----------+--------+--------+----------+--------+\n",
      "|     AMZ|2018-07-25| 1573.08| 1558.64|   1567.61| 1573.08|\n",
      "|     AMZ|2018-07-26| 1578.27| 1556.75|   1566.49| 1566.98|\n",
      "+--------+----------+--------+--------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file to s3\n",
    "final.write.format(\"csv\").option(\"header\",\"true\").mode(\"Overwrite\").save(\"s3://hzhang502/amz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The Mnemonics of the 20 stocks we want to analyze\n",
    "stocks = ['AMZ', 'EBA', 'NFC', 'FB2A', 'MSF', 'TWR', 'DBK', 'DAI', 'CBK', 'ALV', 'BMW', 'AIR', 'VOW3', 'SIE', 'PHIA', 'ADS', 'CON', 'BAS', 'BAYN', '1COV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge the above process in a single function\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc, col\n",
    "\n",
    "def create_csv(stockname):\n",
    "    stock1 = df3.filter(df3['Mnemonic'] == stockname)\n",
    "    stock1.createOrReplaceTempView('stock1')\n",
    "    window = Window.partitionBy(stock1['Date']).orderBy(df['MaxPrice'].desc())\n",
    "    max_df = stock1.select('*', rank().over(window).alias('Max')).filter(col('Max') <= 1)\n",
    "    max_df.createOrReplaceTempView('max')\n",
    "    window1 = Window.partitionBy(stock1['Date']).orderBy(df['MinPrice'].asc())\n",
    "    min_df = stock1.select('*', rank().over(window1).alias('Min')).filter(col('Min') <= 1)\n",
    "    min_df.createOrReplaceTempView('min')\n",
    "    ranked = stock1.withColumn(\"time_rank\",rank().over(Window.partitionBy(stock1['Date']).orderBy(\"Hour\", \"Minute\")))\n",
    "    rankdown = ranked.withColumn(\"time_rankdown\",rank().over(Window.partitionBy(ranked['Date']).orderBy(desc(\"Hour\"), desc(\"Minute\"))))\n",
    "    start_end = rankdown.filter((col('time_rank') == 1) | (col('time_rankdown') == 1))\n",
    "    start_end.createOrReplaceTempView('start_end')\n",
    "    final = spark.sql(\"\"\"SELECT s.Mnemonic, s.Date, a.MaxPrice\n",
    "FROM max a INNER JOIN stock1 s On a.Max == 1 AND s.Date == a.Date\n",
    "GROUP BY s.Mnemonic, s.Date, a.MaxPrice\n",
    "\"\"\")\n",
    "    final.createOrReplaceTempView('final')\n",
    "    final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, i.MinPrice\n",
    "FROM min i INNER JOIN final f On i.Min == 1 AND i.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, i.MinPrice\n",
    "\"\"\")\n",
    "    final.createOrReplaceTempView('final')\n",
    "    final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, s.StartPrice\n",
    "FROM start_end s INNER JOIN final f On s.time_rank == 1 AND s.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, s.StartPrice\n",
    "\"\"\")\n",
    "    final.createOrReplaceTempView('final')\n",
    "    final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, f.StartPrice, s.EndPrice\n",
    "FROM start_end s INNER JOIN final f On s.time_rankdown == 1 AND s.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, f.StartPrice, s.EndPrice\n",
    "\"\"\")\n",
    "    final = final.orderBy('Date')\n",
    "    path = 's3://hzhang502/' + stockname\n",
    "    final.write.format(\"csv\").option(\"header\",\"true\").mode(\"Overwrite\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function on each stock\n",
    "for s in stocks:\n",
    "    create_csv(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "testpd = spark.read.format('csv')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load('s3://hzhang502/amz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+--------+----------+--------+\n",
      "|Mnemonic|               Date|MaxPrice|MinPrice|StartPrice|EndPrice|\n",
      "+--------+-------------------+--------+--------+----------+--------+\n",
      "|     AMZ|2018-07-25 00:00:00| 1573.08| 1558.64|   1567.61| 1573.08|\n",
      "|     AMZ|2018-07-26 00:00:00| 1578.27| 1556.75|   1566.49| 1566.98|\n",
      "+--------+-------------------+--------+--------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testpd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on how to convert the spark df to pandas df\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://apache-spark-developers-list.1001551.n3.nabble.com/Faster-and-Lower-memory-implementation-toPandas-td22869.html\n",
    "def _map_to_pandas(rdds):\n",
    "    \"\"\" Needs to be here due to pickling issues \"\"\"\n",
    "    return [pd.DataFrame(list(rdds))]\n",
    "\n",
    "def toPandas(df, n_partitions=None):\n",
    "    \"\"\"\n",
    "    Returns the contents of `df` as a local `pandas.DataFrame` in a speedy fashion. The DataFrame is\n",
    "    repartitioned if `n_partitions` is passed.\n",
    "    :param df:              pyspark.sql.DataFrame\n",
    "    :param n_partitions:    int or None\n",
    "    :return:                pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if n_partitions is not None: df = df.repartition(n_partitions)\n",
    "    df_pand = df.rdd.mapPartitions(_map_to_pandas).collect()\n",
    "    df_pand = pd.concat(df_pand)\n",
    "    df_pand.columns = df.columns\n",
    "    return df_pand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpd = toPandas(testpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mnemonic</th>\n",
       "      <th>Date</th>\n",
       "      <th>MaxPrice</th>\n",
       "      <th>MinPrice</th>\n",
       "      <th>StartPrice</th>\n",
       "      <th>EndPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZ</td>\n",
       "      <td>2018-07-25</td>\n",
       "      <td>1573.08</td>\n",
       "      <td>1558.64</td>\n",
       "      <td>1567.61</td>\n",
       "      <td>1573.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZ</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>1578.27</td>\n",
       "      <td>1556.75</td>\n",
       "      <td>1566.49</td>\n",
       "      <td>1566.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Mnemonic       Date  MaxPrice  MinPrice  StartPrice  EndPrice\n",
       "0      AMZ 2018-07-25   1573.08   1558.64     1567.61   1573.08\n",
       "0      AMZ 2018-07-26   1578.27   1556.75     1566.49   1566.98"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tech: AMZ EBA NFC FB2A MSF TWR\n",
    "Fin : DBK DAI CBK  ALV (Allianz) \n",
    "Industry: BMW AIR VOW3\n",
    "manufactureï¼šSIE PHIA ADS CON (car parts)\n",
    "Chemical: BAS (basf) BAYN 1COV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

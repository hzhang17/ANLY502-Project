{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pro').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-7-97.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fee8e7f53d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset from s3\n",
    "df = spark.read.format('csv')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load('s3://hzhang502/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+--------+----------+-------------------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|Currency|SecurityID|               Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|TradedVolume|NumberOfTrades|\n",
      "+------------+--------+--------------------+------------+--------+----------+-------------------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|AT0000A0E9W5|    SANT|S+T AG (Z.REG.MK....|Common stock|     EUR|   2504159|2018-02-06 00:00:00|09:00|     20.04|   20.04|   19.91|   19.95|        3314|            16|\n",
      "|AT00000FACC2|     1FC|    FACC AG INH.AKT.|Common stock|     EUR|   2504163|2018-02-06 00:00:00|09:00|      16.5|    16.5|    16.5|    16.5|         250|             2|\n",
      "|AT0000743059|     OMV|              OMV AG|Common stock|     EUR|   2504175|2018-02-06 00:00:00|09:00|      48.8|    48.8|    48.8|    48.8|         164|             5|\n",
      "|AT0000937503|     VAS|      VOESTALPINE AG|Common stock|     EUR|   2504189|2018-02-06 00:00:00|09:00|     49.63|   49.64|   49.63|   49.63|         200|             2|\n",
      "|AT0000969985|     AUS|AT+S AUSTR.T.+SYS...|Common stock|     EUR|   2504191|2018-02-06 00:00:00|09:00|      21.6|    21.6|    21.4|    21.6|         728|             4|\n",
      "+------------+--------+--------------------+------------+--------+----------+-------------------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5) # show first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ISIN: string (nullable = true)\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- SecurityID: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StartPrice: double (nullable = true)\n",
      " |-- MaxPrice: double (nullable = true)\n",
      " |-- MinPrice: double (nullable = true)\n",
      " |-- EndPrice: double (nullable = true)\n",
      " |-- TradedVolume: integer (nullable = true)\n",
      " |-- NumberOfTrades: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # show schema of dataset, some need correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns irrelevant to our analysis\n",
    "drop = ['ISIN', 'Currency', 'SecurityID', 'TradedVolume', 'NumberOfTrades']\n",
    "for col in drop:\n",
    "    df = df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+-------------------+-----+----------+--------+--------+--------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|               Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|\n",
      "+--------+--------------------+------------+-------------------+-----+----------+--------+--------+--------+\n",
      "|    SANT|S+T AG (Z.REG.MK....|Common stock|2018-02-06 00:00:00|09:00|     20.04|   20.04|   19.91|   19.95|\n",
      "|     1FC|    FACC AG INH.AKT.|Common stock|2018-02-06 00:00:00|09:00|      16.5|    16.5|    16.5|    16.5|\n",
      "|     OMV|              OMV AG|Common stock|2018-02-06 00:00:00|09:00|      48.8|    48.8|    48.8|    48.8|\n",
      "|     VAS|      VOESTALPINE AG|Common stock|2018-02-06 00:00:00|09:00|     49.63|   49.64|   49.63|   49.63|\n",
      "|     AUS|AT+S AUSTR.T.+SYS...|Common stock|2018-02-06 00:00:00|09:00|      21.6|    21.6|    21.4|    21.6|\n",
      "+--------+--------------------+------------+-------------------+-----+----------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5) # show first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34947852"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # show total number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast col Date to date type\n",
    "df_new = df.withColumn('Date', df['Date'].cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select stocks only\n",
    "df_new = df_new.filter(df_new['SecurityType'] == 'Common stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate hour and minute to do time range selection later\n",
    "import pyspark\n",
    "import pyspark.sql.functions as fct\n",
    "split_col = fct.split(df_new['Time'], ':')\n",
    "df1 = df_new.withColumn('Hour', split_col.getItem(0))\n",
    "df1 = df1.withColumn('Minute', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|    SANT|S+T AG (Z.REG.MK....|Common stock|2018-02-06|09:00|     20.04|   20.04|   19.91|   19.95|  09|    00|\n",
      "|     1FC|    FACC AG INH.AKT.|Common stock|2018-02-06|09:00|      16.5|    16.5|    16.5|    16.5|  09|    00|\n",
      "|     OMV|              OMV AG|Common stock|2018-02-06|09:00|      48.8|    48.8|    48.8|    48.8|  09|    00|\n",
      "|     VAS|      VOESTALPINE AG|Common stock|2018-02-06|09:00|     49.63|   49.64|   49.63|   49.63|  09|    00|\n",
      "|     AUS|AT+S AUSTR.T.+SYS...|Common stock|2018-02-06|09:00|      21.6|    21.6|    21.4|    21.6|  09|    00|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5) # show first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StartPrice: double (nullable = true)\n",
      " |-- MaxPrice: double (nullable = true)\n",
      " |-- MinPrice: double (nullable = true)\n",
      " |-- EndPrice: double (nullable = true)\n",
      " |-- Hour: string (nullable = true)\n",
      " |-- Minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema() # show schema of dataset, still need to correct Hour and Minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast col Hour and Minute to int \n",
    "df1 = df1.withColumn('Hour', df1['Hour'].cast('int'))\n",
    "df1 = df1.withColumn('Minute', df1['Minute'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.xetra.com/xetra-en/trading/trading-calendar-and-trading-hours\n",
    "# according to xetra, the trading hour for stocks is 9:00-17:30\n",
    "# so we select data in this range only\n",
    "df2 = df1.filter(df1['Hour'] >= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.filter((df2['Hour'] <= 17) | ((df2['Hour'] == 17) & (df2['Minute'] <= 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|Mnemonic|        SecurityDesc|SecurityType|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|Hour|Minute|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "|    SANT|S+T AG (Z.REG.MK....|Common stock|2018-02-06|09:00|     20.04|   20.04|   19.91|   19.95|   9|     0|\n",
      "|     1FC|    FACC AG INH.AKT.|Common stock|2018-02-06|09:00|      16.5|    16.5|    16.5|    16.5|   9|     0|\n",
      "|     OMV|              OMV AG|Common stock|2018-02-06|09:00|      48.8|    48.8|    48.8|    48.8|   9|     0|\n",
      "|     VAS|      VOESTALPINE AG|Common stock|2018-02-06|09:00|     49.63|   49.64|   49.63|   49.63|   9|     0|\n",
      "|     AUS|AT+S AUSTR.T.+SYS...|Common stock|2018-02-06|09:00|      21.6|    21.6|    21.4|    21.6|   9|     0|\n",
      "|    1NBA|ANHEUSER-BUSCH INBEV|Common stock|2018-02-06|09:00|     86.76|   86.76|   86.76|   86.76|   9|     0|\n",
      "|    BBZA|BB BIOTECH NAM.  ...|Common stock|2018-02-06|09:00|     55.85|   55.85|   55.85|   55.85|   9|     0|\n",
      "|     S92|SMA SOLAR TECHNOL.AG|Common stock|2018-02-06|09:00|     44.44|   44.44|   44.34|   44.34|   9|     0|\n",
      "|     ACX|BET-AT-HOME.COM A...|Common stock|2018-02-06|09:00|     89.55|   89.55|   89.55|   89.55|   9|     0|\n",
      "|     MTX|MTU AERO ENGINES ...|Common stock|2018-02-06|09:00|     136.5|   136.6|   136.3|   136.5|   9|     0|\n",
      "+--------+--------------------+------------+----------+-----+----------+--------+--------+--------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(10) # show first 10 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView('df3') # create view of df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Mnemonics of the 20 stocks we want to analyze\n",
    "stocks = ['AMZ', 'EBA', 'NFC', 'FB2A', 'MSF', 'TWR', 'DBK', 'DAI', 'CBK', 'ALV', 'BMW', 'AIR', 'VOW3', 'SIE', 'PHI1', 'ADS', 'CON', 'BAS', 'BAYN', '1COV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc, col\n",
    "\n",
    "# this function will select stock with name stockname,\n",
    "# aggregate the by-minute data, to collect the by-day data of the stock\n",
    "# and save the file to s3\n",
    "def create_csv(stockname):\n",
    "    # first filter out rows for stockname and create view for it\n",
    "    stock1 = df3.filter(df3['Mnemonic'] == stockname)\n",
    "    stock1.createOrReplaceTempView('stock1')\n",
    "    \n",
    "    # use window to select the MaxPrice in a day and store it in max_df\n",
    "    window = Window.partitionBy(stock1['Date']).orderBy(df['MaxPrice'].desc())\n",
    "    max_df = stock1.select('*', rank().over(window).alias('Max')).filter(col('Max') <= 1)\n",
    "    max_df.createOrReplaceTempView('max')\n",
    "    \n",
    "    # use window1 to select the MinPrice in a day and store it in min_df\n",
    "    window1 = Window.partitionBy(stock1['Date']).orderBy(df['MinPrice'].asc())\n",
    "    min_df = stock1.select('*', rank().over(window1).alias('Min')).filter(col('Min') <= 1)\n",
    "    min_df.createOrReplaceTempView('min')\n",
    "    \n",
    "    # use window again to create 2 cols: time_rank an time_rankdown which will\n",
    "    # keep track of the rank of each record according to time\n",
    "    ranked = stock1.withColumn(\"time_rank\",rank().over(Window.partitionBy(stock1['Date']).orderBy(\"Hour\", \"Minute\")))\n",
    "    rankdown = ranked.withColumn(\"time_rankdown\",rank().over(Window.partitionBy(ranked['Date']).orderBy(desc(\"Hour\"), desc(\"Minute\"))))\n",
    "    # and then select the first and last entry in a day\n",
    "    # to get the start and end price of a stock\n",
    "    start_end = rankdown.filter((col('time_rank') == 1) | (col('time_rankdown') == 1))\n",
    "    start_end.createOrReplaceTempView('start_end')\n",
    "    \n",
    "    # join the stock df and max to add max price in a day\n",
    "    final = spark.sql(\"\"\"SELECT s.Mnemonic, s.Date, a.MaxPrice\n",
    "FROM max a INNER JOIN stock1 s On a.Max == 1 AND s.Date == a.Date\n",
    "GROUP BY s.Mnemonic, s.Date, a.MaxPrice\n",
    "\"\"\")\n",
    "    final.createOrReplaceTempView('final')\n",
    "    \n",
    "    # join the resulting df and min to add min price in a day\n",
    "    final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, i.MinPrice\n",
    "FROM min i INNER JOIN final f On i.Min == 1 AND i.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, i.MinPrice\n",
    "\"\"\")\n",
    "    final.createOrReplaceTempView('final')\n",
    "    \n",
    "    # join the resulting df and start_end to add start price in a day\n",
    "    final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, s.StartPrice\n",
    "FROM start_end s INNER JOIN final f On s.time_rank == 1 AND s.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, s.StartPrice\n",
    "\"\"\")\n",
    "    final.createOrReplaceTempView('final')\n",
    "    \n",
    "    # join the resulting df and start_end to add end price in a day\n",
    "    final = spark.sql(\"\"\"SELECT f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, f.StartPrice, s.EndPrice\n",
    "FROM start_end s INNER JOIN final f On s.time_rankdown == 1 AND s.Date == f.Date\n",
    "GROUP BY f.Mnemonic, f.Date, f.MaxPrice, f.MinPrice, f.StartPrice, s.EndPrice\n",
    "\"\"\")\n",
    "    \n",
    "    # order the records by date\n",
    "    final = final.orderBy('Date')\n",
    "    \n",
    "    # store the resulting df to s3\n",
    "    path = 's3://hzhang502/' + stockname\n",
    "    final.write.format(\"csv\").option(\"header\",\"true\").mode(\"Overwrite\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run create_csv on each stock we want to study to get separte csvs in s3\n",
    "for s in stocks:\n",
    "    create_csv(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
